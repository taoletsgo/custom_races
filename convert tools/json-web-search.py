import requests
import json
import threading
import os
import time
import re
import random
import platform
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

# Configurations
MAX_CONCURRENT_REQUESTS = 50
PREFERRED_LANGUAGES = ["en", "ja", "zh", "zh-cn", "fr", "de", "it", "ru", "pt", "pl", "ko", "es", "es-mx"] # Depends on the player's client language when publishing the map
MAIN_FOLDER = "local_files"
SQL_FILE = "import_races.sql"
COMMON_PATTERNS = [(0, 0), (0, 1), (1, 0), (2, 0)]

def clear_console():
	"""Clears the console based on the operating system"""
	if platform.system() == "Windows":
		os.system('cls')
	else:
		os.system('clear')

def ensure_directories_exist():
	"""Creates the folder structure if it doesn't exist"""
	if not os.path.exists(MAIN_FOLDER):
		os.makedirs(MAIN_FOLDER)
		print(f"Main folder '{MAIN_FOLDER}' created.")

	for lang in PREFERRED_LANGUAGES:
		lang_folder = os.path.join(MAIN_FOLDER, f"maps_{lang}")
		if not os.path.exists(lang_folder):
			os.makedirs(lang_folder)
			print(f"Folder '{lang_folder}' created.")

def sanitize_filename(name):
	"""Converts the race name into a valid filename"""
	if not name or not isinstance(name, str):
		return f"unnamed_race_{random.randint(1000, 9999)}"

	sanitized = re.sub(r'[\\/*?:"<>|]', "", name)
	sanitized = re.sub(r'[\s\-\+\.,;=]', "_", sanitized)
	sanitized = re.sub(r'_+', "_", sanitized)
	if len(sanitized) > 100:
		sanitized = sanitized[:100]
	sanitized = sanitized.strip("_")
	return sanitized if sanitized else f"unnamed_race_{random.randint(1000, 9999)}"

def find_value_by_key(obj, key, depth=0, max_depth=10):
	"""Recursively searches for a value by its key in a JSON object"""
	if depth > max_depth:
		return None

	if isinstance(obj, dict):
		if key in obj:
			return obj[key]
		for k, v in obj.items():
			result = find_value_by_key(v, key, depth + 1, max_depth)
			if result is not None:
				return result
	elif isinstance(obj, list):
		for item in obj:
			result = find_value_by_key(item, key, depth + 1, max_depth)
			if result is not None:
				return result

	return None

def extract_race_name(data):
	"""Extracts the race name from the JSON"""
	print("DEBUG - Main keys in JSON:", list(data.keys()) if isinstance(data, dict) else "Not a dictionary")

	race_name = find_value_by_key(data, "nm")
	if race_name and isinstance(race_name, str):
		print(f"DEBUG - Name found in 'nm' field: {race_name}")
		return race_name

	for field in ["name", "title", "Name", "Title"]:
		value = find_value_by_key(data, field)
		if value and isinstance(value, str):
			print(f"DEBUG - Name found in '{field}' field: {value}")
			return value

	random_name = f"unnamed_race_{random.randint(1000, 9999)}"
	print(f"DEBUG - No name found, using: {random_name}")
	return random_name

def get_last_race_id():
	"""Gets the last race ID from the existing SQL file"""
	if not os.path.exists(SQL_FILE):
		return 0

	try:
		with open(SQL_FILE, 'r', encoding='utf-8') as f:
			content = f.read()
			ids = re.findall(r'\((\d+),', content)
			if ids:
				return max(map(int, ids))
			return 0
	except:
		return 0

def generate_sql_import(downloaded_files, image_url):
	"""Generates or updates an SQL file with the downloaded race data"""
	if not downloaded_files:
		return

	last_id = get_last_race_id()
	mode = 'a' if os.path.exists(SQL_FILE) and last_id > 0 else 'w'

	with open(SQL_FILE, mode, encoding='utf-8') as sql_file:
		if mode == 'w':
			sql_file.write("INSERT INTO `custom_race_list` (`raceid`, `route_file`, `route_image`, `category`, `besttimes`) VALUES\n")
		else:
			with open(SQL_FILE, 'r+', encoding='utf-8') as f:
				content = f.read()
				f.seek(0)

				if content.strip().endswith(';'):
					content = content.rsplit(';', 1)[0]

				f.write(content)
				f.truncate()

			sql_file.write(",\n")

		for i, info in enumerate(downloaded_files, start=last_id + 1):
			json_path = os.path.join(MAIN_FOLDER, os.path.basename(info['filename']))
			sql_line = f"({i}, '{json_path}', '{image_url}', 'Custom', '[]')"

			if i < last_id + len(downloaded_files):
				sql_line += ",\n"
			else:
				sql_line += ";\n"

			sql_file.write(sql_line)

	print(f"\nüìÑ SQL file {'updated' if mode == 'a' else 'generated'}: {SQL_FILE}")

def download_json_directly(json_url, image_url=None):
	"""Downloads a JSON file directly from a URL"""
	headers = {
		'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
	}

	try:
		print(f"Downloading JSON directly from: {json_url}")
		response = requests.get(json_url, headers=headers, timeout=5)

		if response.status_code == 200:
			try:
				data = response.json()
				if data:
					parts = json_url.split('/')
					filename = parts[-1]
					lang = filename.split('_')[-1].split('.')[0]

					if lang not in PREFERRED_LANGUAGES:
						lang = PREFERRED_LANGUAGES[0]

					print(f"‚úÖ JSON found (Language: {lang})")

					print("DEBUG - JSON sample:")
					json_str = json.dumps(data, ensure_ascii=False)[:500]
					print(json_str + "..." if len(json_str) >= 500 else json_str)

					race_name = extract_race_name(data)
					print(f"üìù Race name: {race_name}")

					safe_name = sanitize_filename(race_name)
					lang_folder = os.path.join(MAIN_FOLDER, f"maps_{lang}")
					save_filename = os.path.join(lang_folder, f"{safe_name}_{lang}.json")

					with open(save_filename, 'w', encoding='utf-8') as f:
						json.dump(data, f, ensure_ascii=False, indent=2)

					print(f"üìÅ File saved as: {save_filename}")

					if image_url:
						generate_sql_import([{
							'filename': save_filename,
							'race_name': race_name
						}], image_url)

					return {
						'url': json_url,
						'language': lang,
						'race_name': race_name,
						'filename': save_filename
					}

			except json.JSONDecodeError:
				print("‚ùå Error: File is not a valid JSON.")
		else:
			print(f"‚ùå Error: Could not download file (Code: {response.status_code}).")

	except requests.RequestException as e:
		print(f"‚ùå Connection error: {e}")

	return None

def crawl_url(url, found_data, download_info, total_requests, completed_requests, race_names):
	if found_data.is_set():
		with completed_requests.get_lock():
			completed_requests.value += 1
		return False

	headers = {
		'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
	}

	try:
		response = requests.get(url, headers=headers, timeout=5)

		with completed_requests.get_lock():
			completed_requests.value += 1

		if response.status_code == 200:
			try:
				data = response.json()
				if data:
					parts = url.split('/')
					filename = parts[-1]
					lang = filename.split('_')[-1].split('.')[0]

					print(f"\n‚úÖ JSON found: {url} (Language: {lang})")

					if lang in PREFERRED_LANGUAGES:
						print("DEBUG - JSON sample:")
						json_str = json.dumps(data, ensure_ascii=False)[:500]
						print(json_str + "..." if len(json_str) >= 500 else json_str)

						race_name = extract_race_name(data)
						print(f"üìù Race name: {race_name}")

						race_names[lang] = race_name
						safe_name = sanitize_filename(race_name)
						lang_folder = os.path.join(MAIN_FOLDER, f"maps_{lang}")
						save_filename = os.path.join(lang_folder, f"{safe_name}_{lang}.json")

						with open(save_filename, 'w', encoding='utf-8') as f:
							json.dump(data, f, ensure_ascii=False, indent=2)

						print(f"üìÅ File saved as: {save_filename}")

						download_info.append({
							'url': url,
							'language': lang,
							'race_name': race_name,
							'filename': save_filename
						})

					found_data.set()
					return True
			except json.JSONDecodeError:
				pass
		elif response.status_code == 404:
			pass
		elif response.status_code == 429:
			print("\n‚ö†Ô∏è Too many requests. Waiting 2 seconds...")
			time.sleep(2)
		else:
			pass

	except requests.RequestException:
		pass

	progress = int((completed_requests.value / total_requests.value) * 50)
	print(f"\rProgress: [{'#' * progress}{' ' * (50-progress)}] {completed_requests.value}/{total_requests.value} URLs checked", end="")

	return False

def generate_optimized_urls(base_url):
	"""Generates URLs in an optimized order to find results faster"""
	urls = []
	base_path = base_url.rsplit('/', 1)[0]

	for i, j in COMMON_PATTERNS:
		for lang in PREFERRED_LANGUAGES:
			url = f"{base_path}/{i}_{j}_{lang}.json"
			urls.append(url)

	for i in range(2):
		for j in range(500):
			if (i, j) not in COMMON_PATTERNS:
				for lang in PREFERRED_LANGUAGES:
					url = f"{base_path}/{i}_{j}_{lang}.json"
					urls.append(url)

	return urls

def crawl_urls_concurrently(base_url):
	clear_console()
	found_data = threading.Event()
	download_info = []
	race_names = {lang: "" for lang in PREFERRED_LANGUAGES}

	total_requests = multiprocessing.Value('i', 0)
	completed_requests = multiprocessing.Value('i', 0)

	urls = generate_optimized_urls(base_url)

	with total_requests.get_lock():
		total_requests.value = len(urls)

	print(f"Searching for JSON files for {base_url}...")
	print(f"Prioritizing languages: {', '.join(PREFERRED_LANGUAGES)}")
	print(f"Checking {len(urls)} possible URLs with optimized strategy")

	print("Progress: [" + " " * 50 + "] 0/0 URLs checked", end="")

	start_time = time.time()

	with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
		futures = [executor.submit(crawl_url, url, found_data, download_info, total_requests, completed_requests, race_names) for url in urls]
		for future in as_completed(futures):
			if future.result():
				for f in futures:
					f.cancel()
				break

	end_time = time.time()
	duration = end_time - start_time

	print(f"\n\nSearch completed in {duration:.2f} seconds")

	if not download_info:
		print("‚ùå No JSON files found in preferred languages.")
	else:
		print("\nüìã Download summary:")
		for info in download_info:
			print(f"‚úì Race name: {info['race_name']}")
			print(f"File saved as: {info['filename']}")
			print(f"URL: {info['url']}")
			print(f"Language: {info['language']}")

		generate_sql_import(download_info, base_url)

	input("\nPress Enter to continue...")
	clear_console()
	return download_info

def check_mission_exists(base_url):
	"""Quickly checks if the mission exists before searching for JSON files"""
	headers = {
		'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
	}

	try:
		response = requests.head(base_url, headers=headers, timeout=5)
		return response.status_code == 200
	except:
		return False

def is_json_url(url):
	"""Checks if the URL is directly to a JSON file"""
	return url.lower().endswith('.json')

if __name__ == "__main__":
	clear_console()
	ensure_directories_exist()

	while True:
		print("=== Fast GTA V Race JSON Files Downloader ===")
		print("This script will automatically download JSON files and generate/update an SQL for import")
		print(f"Files will be saved in '{MAIN_FOLDER}' and SQL in '{SQL_FILE}'")
		print("Type 'exit' to quit\n")
		base_url = input("\nPaste the image or JSON URL of the race: ")

		if base_url.lower() == 'exit':
			clear_console()
			break

		if not base_url.startswith("http"):
			print("‚ùå Invalid URL. Must start with http:// or https://")
			time.sleep(2)
			clear_console()
			continue

		if is_json_url(base_url):
			image_url = base_url.rsplit('.', 1)[0] + '.jpg'
			result = download_json_directly(base_url, image_url)
			if result:
				print("\nüìã Download summary:")
				print(f"‚úì Race name: {result['race_name']}")
				print(f"File saved as: {result['filename']}")
				print(f"URL: {result['url']}")
				print(f"Language: {result['language']}")

			input("\nPress Enter to continue...")
			clear_console()
		else:
			print("Checking if mission exists...")
			if not check_mission_exists(base_url):
				print("‚ùå The provided URL doesn't seem valid or the mission doesn't exist.")
				time.sleep(2)
				clear_console()
				continue

			crawl_urls_concurrently(base_url)
